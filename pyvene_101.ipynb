{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba6c7e19",
   "metadata": {},
   "source": [
    "# Introduction to pyvene\n",
    "This tutorial shows simple runnable code snippets of how to do different kinds of interventions on neural networks with pyvene."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6994fa",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/stanfordnlp/pyvene/blob/main/pyvene_101.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d123a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zhengxuan Wu\"\n",
    "__version__ = \"01/20/2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26298448-91eb-4cad-85bf-ec5fef436e1d",
   "metadata": {},
   "source": [
    " # Table of Contents  \n",
    "1. [Set-up](#Set-up)     \n",
    "1. [pyvene 101](#pyvene-101) \n",
    "    1. [Get Attention Weights](#Get-Attention-Weights)\n",
    "    1. [Set Activations to Zeros](#Set-Activation-to-Zeros) \n",
    "    1. [Set Activations with Subspaces](#Set-Activations-to-Zeros-with-Subspaces)\n",
    "    1. [Interchange Intervention](#Interchange-Interventions)\n",
    "    1. [Intervention Config](#Intervention-Configuration)\n",
    "    1. [Addition Intervention](#Addition-Intervention)\n",
    "    1. [Trainable Intervention](#Trainable-Intervention)\n",
    "    1. [Activation Collection](#Activation-Collection-with-Intervention)\n",
    "    1. [Activation Collection with Other Intervention](#Activation-Collection-at-Downstream-of-a-Intervened-Model)\n",
    "    1. [Intervene Single Neuron](#Intervene-on-a-Single-Neuron)\n",
    "    1. [Add New Intervention Type](#Add-New-Intervention-Type)\n",
    "    1. [Intervene on Recurrent NNs](#Recurrent-NNs-(Intervene-a-Specific-Timestep))\n",
    "    1. [Intervene across Times with RNNs](#Recurrent-NNs-(Intervene-cross-Time))\n",
    "    1. [Intervene on LM Generation](#LM-Generation)\n",
    "    1. [Saving and Loading](#Saving-and-Loading)\n",
    "    1. [Multi-Source Intervention (Parallel)](#Multi-Source-Interchange-Intervention-(Parallel-Mode))\n",
    "    1. [Multi-Source Intervention (Serial)](#Multi-Source-Interchange-Intervention-(Serial-Mode))\n",
    "    1. [Multi-Source Intervention with Subspaces (Parallel)](#Multi-Source-Interchange-Intervention-with-Subspaces-(Parallel-Mode))\n",
    "    1. [Multi-Source Intervention with Subspaces (Serial)](#Multi-Source-Interchange-Intervention-with-Subspaces-(Serial-Mode))\n",
    "    1. [Interchange Intervention Training](#Interchange-Intervention-Training-(IIT))\n",
    "1. [pyvene 102](#pyvene-102)\n",
    "    1. [Intervention Grouping](#Grouping)\n",
    "    1. [Intervention Skipping](#Intervention-Skipping-in-Runtime)\n",
    "    1. [Subspace Partition](#Subspace-Partition)\n",
    "    1. [Intervention Linking](#Intervention-Linking)\n",
    "    1. [Add New Model Type](#Add-New-Model-Type)\n",
    "    1. [Path Patching](#Composing-Complex-Intervention-Schema:-Path-Patching)\n",
    "    1. [Causal Tracing](#Composing-Complex-Intervention-Schema:-Causal-Tracing-in-15-lines)\n",
    "1. [The End](#The-End)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706e21b",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08304ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyvene\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install git+https://github.com/stanfordnlp/pyvene.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ede4f94",
   "metadata": {},
   "source": [
    "## pyvene 101\n",
    "Before we get started, here are a couple of core notations that are used in this library:\n",
    "- **Base** example: this is the example we are intervening on, or, we are intervening on the computation graph of the model running the **Base** example.\n",
    "- **Source** example or representations: this is the source of our intervention. We use **Source** to intervene on **Base**.\n",
    "- **component**: this is the `nn.module` we are intervening in a pytorch-based NN.\n",
    "- **unit**: this is the axis of our intervention. If we say our **unit** is `pos` (`position`), then you are intervening on each token position.\n",
    "- **unit_locations**: this list gives you the percisely location of your intervention. It is the locations of the unit of analysis you are specifying. For instance, if your `unit` is `pos`, and your `unit_location` is 3, then it means you are intervening on the third token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7245643b-fd44-47a5-a189-ce1565da7e25",
   "metadata": {},
   "source": [
    "### Get Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "17c7f2f6-b0d3-4fe2-8e4f-c044b93f3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import pyvene as pv\n",
    "from circuitsvis.attention import attention_patterns\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "    \"layer\": 3, \"component\": \"attention_weight\", \"unit\": \"h\",\n",
    "    \"intervention_type\": pv.CollectIntervention}, model=gpt2)\n",
    "\n",
    "base = \"When John and Mary went to the shops, Mary gave the book to\"\n",
    "collected_attn_w = pv_gpt2(\n",
    "    base = tokenizer(base, return_tensors=\"pt\"\n",
    "    ), unit_locations={\"base\": [3, 6, 7, 9]}\n",
    ")[0][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7554177-d879-4e7a-968c-c16d1c1a169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-0ab20b33-da06\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-0ab20b33-da06\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\" When\", \" John\", \" and\", \" Mary\", \" went\", \" to\", \" the\", \" shops\", \" ,\", \" Mary\", \" gave\", \" the\", \" book\", \" to\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9854097366333008, 0.01459021307528019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.589033305644989, 0.2578190267086029, 0.15314757823944092, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26736193895339966, 0.5403783321380615, 0.13308368623256683, 0.05917603150010109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6825149655342102, 0.01780739426612854, 0.13987505435943604, 0.11463738977909088, 0.04516516253352165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5609337091445923, 0.0013905840460211039, 0.004899783059954643, 0.0009968421654775739, 0.28233951330184937, 0.14943958818912506, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14053699374198914, 0.015544748865067959, 0.0016953239683061838, 0.012137382291257381, 0.08979184180498123, 0.7284666299819946, 0.011827105656266212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5724174380302429, 0.007193893194198608, 0.004671263974159956, 0.0005323648802004755, 0.09833575785160065, 0.18908782303333282, 0.10280511528253555, 0.024956319481134415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7340760231018066, 0.00130399817135185, 0.00025182595709338784, 0.0004722768208011985, 0.01291073951870203, 0.034565430134534836, 0.023494353517889977, 0.09078366309404373, 0.10214166343212128, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6239180564880371, 0.004226807039231062, 0.0004074262687936425, 0.000215196079807356, 0.0070261768996715546, 0.0016022950876504183, 0.0015535561833530664, 0.0084286630153656, 0.2421010434627533, 0.11052078753709793, 0.0, 0.0, 0.0, 0.0], [0.7305745482444763, 9.635860624257475e-05, 0.0004672691866289824, 0.00014692667173221707, 0.0003504059568513185, 0.00024298939388245344, 0.0003539719036780298, 0.0006368626491166651, 0.2088237702846527, 0.026408210396766663, 0.03189868852496147, 0.0, 0.0, 0.0], [0.48910295963287354, 0.00015079000149853528, 2.2855663701193407e-05, 6.551677506649867e-05, 0.0014974122168496251, 0.0030591695103794336, 8.660172898089513e-05, 0.0018210692796856165, 0.023243380710482597, 0.01170489564538002, 0.4381686747074127, 0.031076675280928612, 0.0, 0.0], [0.7656187415122986, 2.5783270757528953e-05, 5.341587893781252e-05, 4.337206064519705e-06, 0.0009714715415611863, 0.0014613530365750194, 0.0012123757041990757, 0.0015804852591827512, 0.01396941114217043, 0.0003027132188435644, 0.05894956737756729, 0.14235945045948029, 0.013490859419107437, 0.0], [0.718644380569458, 2.0551024135784246e-06, 1.6822117459014407e-06, 6.772072538296925e-07, 0.0006485995836555958, 8.166622865246609e-05, 0.00017981593555305153, 0.00024144693452399224, 0.005939781200140715, 0.00020662747556343675, 0.07487449795007706, 0.04490349441766739, 0.023216934874653816, 0.13105835020542145]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5446731448173523, 0.4553268253803253, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.255667507648468, 0.6852033138275146, 0.05912914127111435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16357244551181793, 0.7031223177909851, 0.04475100338459015, 0.08855421841144562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0703035518527031, 0.42228347063064575, 0.1253787726163864, 0.35021790862083435, 0.03181629627943039, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20763106644153595, 0.23636876046657562, 0.1215662807226181, 0.18338900804519653, 0.2087305635213852, 0.04231427237391472, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18833871185779572, 0.18984492123126984, 0.03153364732861519, 0.18443730473518372, 0.19912105798721313, 0.14717243611812592, 0.05955188348889351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2112695276737213, 0.15846143662929535, 0.034646324813365936, 0.04805667698383331, 0.08330892771482468, 0.1020636186003685, 0.17369456589221954, 0.18849895894527435, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10576264560222626, 0.3814305067062378, 0.057179611176252365, 0.0889677181839943, 0.03304164856672287, 0.04288182407617569, 0.09064100682735443, 0.13087010383605957, 0.06922498345375061, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3460056781768799, 0.06245953589677811, 0.005141682922840118, 0.007990117184817791, 0.006614839192479849, 0.01294900942593813, 0.004647658206522465, 0.02060820907354355, 0.08327481150627136, 0.4503084421157837, 0.0, 0.0, 0.0, 0.0], [0.045171286910772324, 0.0097715575248003, 0.0044892760924994946, 0.010263421572744846, 0.002007345436140895, 0.0007631033658981323, 0.00042492180364206433, 0.0015386984450742602, 0.028623733669519424, 0.877822995185852, 0.019123706966638565, 0.0, 0.0, 0.0], [0.23565833270549774, 0.010141832754015923, 0.0017039569793269038, 0.011048211716115475, 0.013082125224173069, 0.0027215604204684496, 0.001049292040988803, 0.01958981156349182, 0.028953038156032562, 0.4413183331489563, 0.17850013077259064, 0.05623333901166916, 0.0, 0.0], [0.15714770555496216, 0.010133255273103714, 0.0022940251510590315, 0.013752499595284462, 0.006110805552452803, 0.0022729397751390934, 0.007145897950977087, 0.019356872886419296, 0.04362756386399269, 0.3378215730190277, 0.10041102021932602, 0.17758363485336304, 0.12234228849411011, 0.0], [0.11416812241077423, 0.0031193161848932505, 0.000720214331522584, 0.002777661429718137, 0.004747633822262287, 0.0005136606632731855, 0.002278262283653021, 0.017003698274493217, 0.03408767655491829, 0.14352773129940033, 0.06584064662456512, 0.241251602768898, 0.244459867477417, 0.12550392746925354]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9812337160110474, 0.018766330555081367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2480641007423401, 0.6809360980987549, 0.07099977135658264, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3986380994319916, 0.3664582371711731, 0.18201391398906708, 0.05288971588015556, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17075878381729126, 0.03581927716732025, 0.032716281712055206, 0.6968552470207214, 0.06385044753551483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3672485649585724, 0.022808516398072243, 0.011264393106102943, 0.23542538285255432, 0.22670550644397736, 0.13654755055904388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.27717506885528564, 0.01908276416361332, 0.0034347253385931253, 0.028242843225598335, 0.32528355717658997, 0.2633170187473297, 0.08346399664878845, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35166066884994507, 0.024227149784564972, 0.008686970919370651, 0.0146140456199646, 0.07679355889558792, 0.11349321156740189, 0.22562381625175476, 0.18490058183670044, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15937264263629913, 0.0024997745640575886, 0.0004172378103248775, 0.006049123127013445, 0.05982532724738121, 0.03189953789114952, 0.03587666526436806, 0.5827204585075378, 0.12133921682834625, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4403255581855774, 0.002526576165109873, 0.0005072358762845397, 0.00022468948736786842, 0.031196745112538338, 0.013982697390019894, 0.023780426010489464, 0.1715550273656845, 0.22699525952339172, 0.08890575915575027, 0.0, 0.0, 0.0, 0.0], [0.27150100469589233, 0.0007035345770418644, 0.0004126241838093847, 0.006875908933579922, 0.006028894335031509, 0.0009806690504774451, 0.0011759009212255478, 0.00645709503442049, 0.13034427165985107, 0.5247321724891663, 0.050787873566150665, 0.0, 0.0, 0.0], [0.3586622476577759, 0.00015453396190423518, 8.724456165509764e-06, 0.00012197054456919432, 0.0016864046920090914, 0.0006881994195282459, 0.00022317987168207765, 0.018456002697348595, 0.03499250486493111, 0.02970929816365242, 0.4730148911476135, 0.08228205144405365, 0.0, 0.0], [0.4470760226249695, 0.0005217022262513638, 4.2492996726650745e-05, 0.00017623938038013875, 0.0012595856096595526, 0.0008753990987315774, 0.0024912268854677677, 0.012827492319047451, 0.03302663192152977, 0.009287695400416851, 0.16146676242351532, 0.23268505930900574, 0.09826371818780899, 0.0], [0.10403405874967575, 1.2090370546502527e-05, 7.153685146477073e-07, 1.7513881175545976e-05, 5.965174932498485e-05, 1.283893288928084e-05, 0.00012807571329176426, 0.004819913767278194, 0.004880167078226805, 0.005943886004388332, 0.07023722678422928, 0.03629051148891449, 0.7245781421661377, 0.048985112458467484]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9456595182418823, 0.05434050410985947, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8176350593566895, 0.054158616811037064, 0.12820623815059662, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.33012330532073975, 0.30497869849205017, 0.21968455612659454, 0.14521346986293793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7842373251914978, 0.03275317698717117, 0.10478043556213379, 0.009312799200415611, 0.06891629844903946, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4177980124950409, 0.010381350293755531, 0.02169383503496647, 0.004745353013277054, 0.5328279137611389, 0.012553559616208076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1568845957517624, 0.03222978115081787, 0.020829541608691216, 0.015901589766144753, 0.7223911881446838, 0.03273959085345268, 0.019023669883608818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23264949023723602, 0.013474684208631516, 0.032830484211444855, 0.004947020206600428, 0.5156400799751282, 0.12778788805007935, 0.052270907908678055, 0.02039949782192707, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5498018860816956, 0.04505528509616852, 0.060185473412275314, 0.025247331708669662, 0.1646505743265152, 0.039481308311223984, 0.017684374004602432, 0.02022070437669754, 0.07767309248447418, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5410144925117493, 0.005918112117797136, 0.011908983811736107, 0.00948334764689207, 0.012190600857138634, 0.015611104667186737, 0.015373452566564083, 0.00397510128095746, 0.15732617676258087, 0.22719861567020416, 0.0, 0.0, 0.0, 0.0], [0.811376690864563, 0.0024615370202809572, 0.004642462823539972, 0.0014555070083588362, 0.011929330416023731, 0.014251787215471268, 0.005814396310597658, 0.004723686259239912, 0.10774046182632446, 0.019029168412089348, 0.016574963927268982, 0.0, 0.0, 0.0], [0.3461150527000427, 0.0009624569211155176, 0.001887769903987646, 0.0009809614857658744, 0.012113552540540695, 0.010828105732798576, 0.0020885507110506296, 0.002414716873317957, 0.03301374986767769, 0.015389048494398594, 0.5274377465248108, 0.04676833748817444, 0.0, 0.0], [0.36297789216041565, 0.001092755701392889, 0.002352980663999915, 0.0005529641639441252, 0.013964071869850159, 0.020861303433775902, 0.011667135171592236, 0.013111627660691738, 0.048277586698532104, 0.008838274516165257, 0.3459053933620453, 0.11185123771429062, 0.05854679271578789, 0.0], [0.22621719539165497, 0.00037293718196451664, 0.0006010105134919286, 0.00017127972387243062, 0.017986273393034935, 0.0003307470178697258, 0.0029637711122632027, 0.0022913007996976376, 0.045134227722883224, 0.003631867002695799, 0.6085795164108276, 0.0665958821773529, 0.013582262210547924, 0.011541828513145447]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f2411760c40>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_patterns(\n",
    "    tokens=[\" \"+t.lstrip(\"Ä \") for t in tokenizer.tokenize(base)], \n",
    "    attention=collected_attn_w\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5addb-4bd7-4129-b350-0677774f5790",
   "metadata": {},
   "source": [
    "### Set Activation to Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82664f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "# define the component to zero-out\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "    \"layer\": 0, \"component\": \"mlp_output\",\n",
    "    \"source_representation\": torch.zeros(gpt2.config.n_embd)\n",
    "}, model=gpt2)\n",
    "# run the intervened forward pass\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"), \n",
    "    # we define the intervening token dynamically\n",
    "    unit_locations={\"base\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39071858",
   "metadata": {},
   "source": [
    "### Set Activations to Zeros with Subspaces\n",
    "The notion of subspace means the actual dimensions you are intervening. If we have a representation in a size of 512, the first 128 activation values are its subspace activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7896c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "# built-in helper to get a HuggingFace model\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "# create with dict-based config\n",
    "pv_config = pv.IntervenableConfig({\n",
    "  \"layer\": 0, \"component\": \"mlp_output\"})\n",
    "#initialize model\n",
    "pv_gpt2 = pv.IntervenableModel(pv_config, model=gpt2)\n",
    "# run an intervened forward pass\n",
    "intervened_outputs = pv_gpt2(\n",
    "  # the intervening base input\n",
    "  base=tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"), \n",
    "  # the location to intervene at (3rd token)\n",
    "  unit_locations={\"base\": 3},\n",
    "  # the individual dimensions targetted\n",
    "  subspaces=[10,11,12],\n",
    "  source_representations=torch.zeros(gpt2.config.n_embd)\n",
    ")\n",
    "# sharing\n",
    "pv_gpt2.save(\"./tmp/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1410904d",
   "metadata": {},
   "source": [
    "### Interchange Interventions\n",
    "Instead of a static vector, we can intervene the model with activations sampled from a different forward run. We call this interchange intervention, where intervention happens between two examples and we are interchanging activations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9691c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "# built-in helper to get a HuggingFace model\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "# create with dict-based config\n",
    "pv_config = pv.IntervenableConfig({\n",
    "  \"layer\": 0,\n",
    "  \"component\": \"mlp_output\"},\n",
    "  intervention_types=pv.VanillaIntervention\n",
    ")\n",
    "#initialize model\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "  pv_config, model=gpt2)\n",
    "# run an interchange intervention \n",
    "intervened_outputs = pv_gpt2(\n",
    "  # the base input\n",
    "  base=tokenizer(\n",
    "    \"The capital of Spain is\", \n",
    "    return_tensors = \"pt\"), \n",
    "  # the source input\n",
    "  sources=tokenizer(\n",
    "    \"The capital of Italy is\", \n",
    "    return_tensors = \"pt\"), \n",
    "  # the location to intervene at (3rd token)\n",
    "  unit_locations={\"sources->base\": 3},\n",
    "  # the individual dimensions targeted\n",
    "  subspaces=[10,11,12]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c890fda4",
   "metadata": {},
   "source": [
    "### Intervention Configuration\n",
    "You can also initialize the config without the lazy dictionary passing by enabling more options, e.g., the mode of these interventions are executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa3e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "# standalone configuration object\n",
    "config = pv.IntervenableConfig([\n",
    "    {\n",
    "        \"layer\": _,\n",
    "        \"component\": \"mlp_output\",\n",
    "        \"source_representation\": torch.zeros(\n",
    "            gpt2.config.n_embd)\n",
    "    } for _ in range(4)],\n",
    "    mode=\"parallel\"\n",
    ")\n",
    "# this object is serializable\n",
    "print(config)\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\"), \n",
    "    unit_locations={\"base\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b2270",
   "metadata": {},
   "source": [
    "### Addition Intervention\n",
    "Activation swap is one kind of interventions we can perform. Here is another simple one: `pv.AdditionIntervention`, which adds the sampled representation into the **Base** run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a40f5989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 0,\n",
    "    \"component\": \"mlp_input\"},\n",
    "    pv.AdditionIntervention\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The Space Needle is in downtown\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    unit_locations={\"base\": [0, 1, 2, 3]},\n",
    "    source_representations = torch.rand(gpt2.config.n_embd)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099ddf77",
   "metadata": {},
   "source": [
    "### Trainable Intervention\n",
    "Interventions can contain trainable parameters, and hook-up with the model to receive gradients end-to-end. They are often useful in searching for an particular interpretation of the representation.\n",
    "\n",
    "The following example does a single step gradient calculation to push the model to generate `Rome` after the intervention. If we can train such intervention at scale with low loss, it means you have a causal grab onto your model. In terms of interpretability, that means, somehow you find a representation (not the original one since its trained) that maps onto the `capital` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f058ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "das_config = pv.IntervenableConfig({\n",
    "    \"layer\": 8,\n",
    "    \"component\": \"block_output\",\n",
    "    \"low_rank_dimension\": 1},\n",
    "    # this is a trainable low-rank rotation\n",
    "    pv.LowRankRotatedSpaceIntervention\n",
    ")\n",
    "\n",
    "das_gpt2 = pv.IntervenableModel(das_config, model=gpt2)\n",
    "\n",
    "last_hidden_state = das_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    sources = tokenizer(\n",
    "        \"The capital of Italy is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    unit_locations={\"sources->base\": 3}\n",
    ")[-1].last_hidden_state[:,-1]\n",
    "\n",
    "# golden counterfacutual label as Rome\n",
    "label = tokenizer.encode(\n",
    "    \" Rome\", return_tensors=\"pt\")\n",
    "logits = torch.matmul(\n",
    "    last_hidden_state, gpt2.wte.weight.t())\n",
    "\n",
    "m = torch.nn.CrossEntropyLoss()\n",
    "loss = m(logits, label.view(-1))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd2b8e",
   "metadata": {},
   "source": [
    "### Activation Collection with Intervention\n",
    "You can also collect activations with our provided `pv.CollectIntervention` intervention. More importantly, this can be used interchangably with other interventions. You can collect something from an intervened model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bd585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"intervention_type\": pv.CollectIntervention}\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    config, model=gpt2)\n",
    "\n",
    "collected_activations = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), unit_locations={\"sources->base\": 3}\n",
    ")[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0d0c6",
   "metadata": {},
   "source": [
    "### Activation Collection at Downstream of a Intervened Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfcb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 8,\n",
    "    \"component\": \"block_output\",\n",
    "    \"intervention_type\": pv.VanillaIntervention}\n",
    ")\n",
    "\n",
    "config.add_intervention({\n",
    "    \"layer\": 10,\n",
    "    \"component\": \"block_output\",\n",
    "    \"intervention_type\": pv.CollectIntervention})\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    config, model=gpt2)\n",
    "\n",
    "collected_activations = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    sources = [tokenizer(\n",
    "        \"The capital of Italy is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), None], unit_locations={\"sources->base\": 3}\n",
    ")[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e6e4d9",
   "metadata": {},
   "source": [
    "### Intervene on a Single Neuron\n",
    "We want to provide a good user interface so that interventions can be done easily by people with less pytorch or programming experience. Meanwhile, we also want to be flexible and provide the depth of control required for highly specific tasks. Here is an example where we intervene on a specific neuron at a specific head of a layer in a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b6401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig({\n",
    "    \"layer\": 8,\n",
    "    \"component\": \"head_attention_value_output\",\n",
    "    \"unit\": \"h.pos\",\n",
    "    \"intervention_type\": pv.CollectIntervention}\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    config, model=gpt2)\n",
    "\n",
    "collected_activations = pv_gpt2(\n",
    "    base = tokenizer(\n",
    "        \"The capital of Spain is\", \n",
    "        return_tensors=\"pt\"\n",
    "    ), \n",
    "    unit_locations={\n",
    "        # GET_LOC is a helper.\n",
    "        # (3,3) means head 3 position 3\n",
    "        \"base\": pv.GET_LOC((3,3))\n",
    "    },\n",
    "    # the notion of subspace is used to target neuron 0.\n",
    "    subspaces=[0]\n",
    ")[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692bc15",
   "metadata": {},
   "source": [
    "### Add New Intervention Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1597221a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "class MultiplierIntervention(\n",
    "  pv.ConstantSourceIntervention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "    def forward(\n",
    "    self, base, source=None, subspaces=None):\n",
    "        return base * 99.0\n",
    "# run with new intervention type\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "  \"intervention_type\": MultiplierIntervention}, \n",
    "  model=gpt2)\n",
    "intervened_outputs = pv_gpt2(\n",
    "  base = tokenizer(\"The capital of Spain is\", \n",
    "    return_tensors=\"pt\"), \n",
    "  unit_locations={\"base\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079050f6",
   "metadata": {},
   "source": [
    "### Recurrent NNs (Intervene a Specific Timestep)\n",
    "Existing intervention libraries focus on Transformer models. They often lack of supports for GRUs, LSTMs or any state-space model. The fundemental problem is in the hook mechanism provided by PyTorch. Hook is attached to a module before runtime. Models like GRUs will lead to undesired callback from the hook as there is no notion of state or time of the hook. \n",
    "\n",
    "We make our hook stateful, so you can intervene on recurrent NNs like GRUs. This notion of time will become useful when intervening on Transformers yet want to unroll the causal effect during generation as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, _, gru = pv.create_gru_classifier(\n",
    "    pv.GRUConfig(h_dim=32))\n",
    "\n",
    "pv_gru = pv.IntervenableModel({\n",
    "    \"component\": \"cell_output\",\n",
    "    \"unit\": \"t\", \n",
    "    \"intervention_type\": pv.ZeroIntervention},\n",
    "    model=gru)\n",
    "\n",
    "rand_t = torch.rand(1,10, gru.config.h_dim)\n",
    "\n",
    "intervened_outputs = pv_gru(\n",
    "  base = {\"inputs_embeds\": rand_t}, \n",
    "  unit_locations={\"base\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031dd5de",
   "metadata": {},
   "source": [
    "### Recurrent NNs (Intervene cross Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48166c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "# built-in helper to get a GRU\n",
    "_, _, gru = pv.create_gru_classifier(\n",
    "    pv.GRUConfig(h_dim=32))\n",
    "# wrap it with config\n",
    "pv_gru = pv.IntervenableModel({\n",
    "    \"component\": \"cell_output\",\n",
    "    # intervening on time\n",
    "    \"unit\": \"t\", \n",
    "    \"intervention_type\": pv.ZeroIntervention},\n",
    "    model=gru)\n",
    "# run an intervened forward pass\n",
    "rand_b = torch.rand(1,10, gru.config.h_dim)\n",
    "rand_s = torch.rand(1,10, gru.config.h_dim)\n",
    "intervened_outputs = pv_gru(\n",
    "  base = {\"inputs_embeds\": rand_b}, \n",
    "  sources = [{\"inputs_embeds\": rand_s}], \n",
    "  # intervening time step\n",
    "  unit_locations={\"sources->base\": (6, 3)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121366c1",
   "metadata": {},
   "source": [
    "### LMs Generation\n",
    "You can also intervene the generation call of LMs. Here is a simple example where we try to add a vector into the MLP output when the model decodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718e2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "# built-in helper to get tinystore\n",
    "_, tokenizer, tinystory = pv.create_gpt_neo()\n",
    "emb_happy = tinystory.transformer.wte(\n",
    "    torch.tensor(14628)) \n",
    "\n",
    "pv_tinystory = pv.IntervenableModel([{\n",
    "    \"layer\": l,\n",
    "    \"component\": \"mlp_output\",\n",
    "    \"intervention_type\": pv.AdditionIntervention\n",
    "    } for l in range(tinystory.config.num_layers)],\n",
    "    model=tinystory\n",
    ")\n",
    "# prompt and generate\n",
    "prompt = tokenizer(\n",
    "    \"Once upon a time there was\", return_tensors=\"pt\")\n",
    "_, intervened_story = pv_tinystory.generate(\n",
    "    tokenizer(\"Once upon a time there was\", return_tensors=\"pt\"),\n",
    "    source_representations=emb_happy*0.3, max_length=256\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(\n",
    "    intervened_story[0], \n",
    "    skip_special_tokens=True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb539f4b",
   "metadata": {},
   "source": [
    "### Saving and Loading\n",
    "This is one of the benefits of program abstraction. We abstract out the intervention and its schema, so we have a user friendly interface. Furthermore, it allows us to have a serializable configuration file that tells everything about your configuration.\n",
    "\n",
    "You can then save, share and load interventions easily. Note that you still need your access to the data, if you need to sample **Source** representations from other examples. But we think this is doable via a separate HuggingFace datasets upload. In the future, there could be an option of coupling this configuration with a specific remote dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f3773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "# run with new intervention type\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "  \"intervention_type\": pv.ZeroIntervention}, \n",
    "  model=gpt2)\n",
    "\n",
    "pv_gpt2.save(\"./tmp/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b894b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_gpt2 = pv.IntervenableModel.load(\n",
    "    \"./tmp/\",\n",
    "    model=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d07ca8",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention (Parallel Mode)\n",
    "\n",
    "What is multi-source? In the examples above, interventions are at most across two examples. We support interventions across many examples. You can sample representations from two inputs, and plut them into a single **Base**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "847410a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n",
      "_the                 0.07233363389968872\n",
      "_a                   0.05731499195098877\n",
      "_not                 0.04443885385990143\n",
      "_Italian             0.033642884343862534\n",
      "_often               0.024385808035731316\n",
      "_called              0.022171705961227417\n",
      "_known               0.017808808013796806\n",
      "_that                0.016059240326285362\n",
      "_\"                   0.012973357923328876\n",
      "_an                  0.012878881767392159\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "parallel_config = pv.IntervenableConfig([\n",
    "  {\"layer\": 3, \"component\": \"block_output\"},\n",
    "  {\"layer\": 3, \"component\": \"block_output\"}],\n",
    "  # intervene on base at the same time\n",
    "  mode=\"parallel\")\n",
    "parallel_gpt2 = pv.IntervenableModel(\n",
    "  parallel_config, model=gpt2)\n",
    "base = tokenizer(\n",
    "  \"The capital of Spain is\", \n",
    "  return_tensors=\"pt\")\n",
    "sources = [\n",
    "  tokenizer(\"The language of Spain is\", \n",
    "    return_tensors=\"pt\"),\n",
    "  tokenizer(\"The capital of Italy is\", \n",
    "    return_tensors=\"pt\")]\n",
    "intervened_outputs = parallel_gpt2(\n",
    "    base, sources,\n",
    "    {\"sources->base\": (\n",
    "    # each list has a dimensionality of\n",
    "    # [num_intervention, batch, num_unit]\n",
    "    [[[1]],[[3]]],  [[[1]],[[3]]])}\n",
    ")\n",
    "\n",
    "distrib = pv.embed_to_distrib(\n",
    "    gpt2, intervened_outputs[1].last_hidden_state, logits=False)\n",
    "pv.top_vals(tokenizer, distrib[0][-1], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93402c",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention (Serial Mode)\n",
    "\n",
    "Or you can do them sequentially, where you intervene among your **Source** examples, and get some intermediate states before merging the activations into the **Base** run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5752dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pv.IntervenableConfig([\n",
    "  {\"layer\": 3, \"component\": \"block_output\"},\n",
    "  {\"layer\": 10, \"component\": \"block_output\"}],\n",
    "  # intervene on base one after another\n",
    "  mode=\"serial\")\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "  config, model=gpt2)\n",
    "base = tokenizer(\n",
    "  \"The capital of Spain is\", \n",
    "  return_tensors=\"pt\")\n",
    "sources = [\n",
    "  tokenizer(\"The language of Spain is\", \n",
    "    return_tensors=\"pt\"),\n",
    "  tokenizer(\"The capital of Italy is\", \n",
    "    return_tensors=\"pt\")]\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources,\n",
    "    # intervene in serial at two positions\n",
    "    {\"source_0->source_1\": 1, \n",
    "     \"source_1->base\"    : 4})\n",
    "\n",
    "distrib = pv.embed_to_distrib(\n",
    "    gpt2, intervened_outputs[1].last_hidden_state, logits=False)\n",
    "pv.top_vals(tokenizer, distrib[0][-1], n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28621880",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention with Subspaces (Parallel Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773aba2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": \n",
    "         [[0, 128], [128, 256]]}]*2,\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    "    # act in parallel\n",
    "    mode=\"parallel\"\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"),\n",
    "          tokenizer(\"The capital of China is\", return_tensors=\"pt\")]\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources,\n",
    "    # on same position\n",
    "    {\"sources->base\": 4},\n",
    "    # on different subspaces\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7223603f",
   "metadata": {},
   "source": [
    "### Multi-Source Interchange Intervention with Subspaces (Serial Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]]},\n",
    "    {\"layer\": 2, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]]}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    "    # act in parallel\n",
    "    mode=\"serial\"\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"),\n",
    "          tokenizer(\"The capital of China is\", return_tensors=\"pt\")]\n",
    "\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources,\n",
    "    # serialized intervention\n",
    "    # order is based on sources list\n",
    "    {\"source_0->source_1\": 3, \"source_1->base\": 4},\n",
    "    # on different subspaces\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5fcb37",
   "metadata": {},
   "source": [
    "### Interchange Intervention Training (IIT)\n",
    "Interchange intervention training (IIT) is a technique of inducing causal structures into neural models. This library naturally supports this. By training IIT, you can simply turn the gradient on for the wrapping model. In this way, your model can be trained with your interventional signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7dde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel({\n",
    "    \"layer\": 8}, \n",
    "    model=gpt2\n",
    ")\n",
    "\n",
    "pv_gpt2.enable_model_gradients()\n",
    "# run counterfactual forward as usual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c7ccad",
   "metadata": {},
   "source": [
    "## pyvene 102\n",
    "Now, you are pretty familiar with pyvene basic APIs. There are more to come. We support all sorts of weird interventions, and we encapsulate them as objects so that, even they are super weird (e.g., nested, multiple locations, different types), you can share them easily with others. BTW, if the intervention is trainable, the artifacts will be saved and shared as well.\n",
    "\n",
    "With that, here are a couple of additional APIs.\n",
    "\n",
    "### Grouping\n",
    "\n",
    "You can group interventions together so that they always receive the same input when you want to use them to get activations at different places. Here is an example, where you are taking in the same **Source** example, you fetch activations twice: once in position 3 and layer 0, once in position 4 and layer 2. You don't have to pass in another dummy **Source**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84afd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    {\"layer\": 0, \"component\": \"block_output\", \"group_key\": 0},\n",
    "    {\"layer\": 2, \"component\": \"block_output\", \"group_key\": 0}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")]\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, sources, \n",
    "    {\"sources->base\": ([\n",
    "        [[3]], [[4]] # these two are for two interventions\n",
    "    ], [             # source position 3 into base position 4\n",
    "        [[3]], [[4]] \n",
    "    ])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aeb892",
   "metadata": {},
   "source": [
    "### Intervention Skipping in Runtime\n",
    "You may configure a lot of interventions, but during training, not every example will have to use all of them. So, you can skip interventions for different examples differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cd8fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    # these are equivalent interventions\n",
    "    # we create them on purpose\n",
    "    {\"layer\": 0, \"component\": \"block_output\"},\n",
    "    {\"layer\": 0, \"component\": \"block_output\"},\n",
    "    {\"layer\": 0, \"component\": \"block_output\"}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")\n",
    "# skipping 1, 2 and 3\n",
    "_, pv_out1 = pv_gpt2(base, [None, None, source],\n",
    "    {\"sources->base\": ([None, None, [[4]]], [None, None, [[4]]])})\n",
    "_, pv_out2 = pv_gpt2(base, [None, source, None],\n",
    "    {\"sources->base\": ([None, [[4]], None], [None, [[4]], None])})\n",
    "_, pv_out3 = pv_gpt2(base, [source, None, None],\n",
    "    {\"sources->base\": ([[[4]], None, None], [[[4]], None, None])})\n",
    "# should have the same results\n",
    "print(\n",
    "    torch.equal(pv_out1.last_hidden_state, pv_out2.last_hidden_state),\n",
    "    torch.equal(pv_out2.last_hidden_state, pv_out3.last_hidden_state)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9df6acd",
   "metadata": {},
   "source": [
    "### Subspace Partition\n",
    "You can partition your subspace before hand. If you don't, the library assumes you each neuron is in its own subspace. In this example, you partition your subspace into two continous chunk, `[0, 128), [128,256)`, which means all the neurons from index 0 upto 127 are along to partition 1. During runtime, you can intervene on all the neurons in the same parition together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a66bbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    # they are linked to manipulate the same representation\n",
    "    # but in different subspaces\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     # subspaces can be partitioned into continuous chunks\n",
    "     # [i, j] are the boundary indices\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]]}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")\n",
    "\n",
    "# using intervention skipping for subspace\n",
    "intervened_outputs = pv_gpt2(\n",
    "    base, [source],\n",
    "    {\"sources->base\": 4},\n",
    "    # intervene only only dimensions from 128 to 256\n",
    "    subspaces=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdde257",
   "metadata": {},
   "source": [
    "### Intervention Linking\n",
    "Interventions can be linked to share weights and share subspaces. Here is an example of how to link interventions together. If interventions are trainable, then their weights are tied as well.\n",
    "\n",
    "Why this is useful? it is because sometimes, you may want to intervene on different subspaces differently. Say you have a representation in a size of 512, and you hypothesize the first half represents A, and the second half represents B, you can then use the subspace intervention to test it out. With trainable interventions, you can also optimize your interventions on the same representation yet with different subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec19da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "config = pv.IntervenableConfig([\n",
    "    # they are linked to manipulate the same representation\n",
    "    # but in different subspaces\n",
    "    {\"layer\": 0, \"component\": \"block_output\", \n",
    "     \"subspace_partition\": [[0, 128], [128, 256]], \"intervention_link_key\": 0},\n",
    "    {\"layer\": 0, \"component\": \"block_output\",\n",
    "     \"subspace_partition\": [[0, 128], [128, 256]], \"intervention_link_key\": 0}],\n",
    "    intervention_types=pv.VanillaIntervention,\n",
    ")\n",
    "pv_gpt2 = pv.IntervenableModel(config, model=gpt2)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")\n",
    "\n",
    "# using intervention skipping for subspace\n",
    "_, pv_out1 = pv_gpt2(\n",
    "    base, [None, source],\n",
    "    # 4 means token position 4\n",
    "    {\"sources->base\": ([None, [[4]]], [None, [[4]]])},\n",
    "    # 1 means the second partition in the config\n",
    "    subspaces=[None, [[1]]],\n",
    ")\n",
    "_, pv_out2 = pv_gpt2(\n",
    "    base,\n",
    "    [source, None],\n",
    "    {\"sources->base\": ([[[4]], None], [[[4]], None])},\n",
    "    subspaces=[[[1]], None],\n",
    ")\n",
    "print(torch.equal(pv_out1.last_hidden_state, pv_out2.last_hidden_state))\n",
    "\n",
    "# subspaces provide a list of index and they can be in any order\n",
    "_, pv_out3 = pv_gpt2(\n",
    "    base,\n",
    "    [source, source],\n",
    "    {\"sources->base\": ([[[4]], [[4]]], [[[4]], [[4]]])},\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")\n",
    "_, pv_out4 = pv_gpt2(\n",
    "    base,\n",
    "    [source, source],\n",
    "    {\"sources->base\": ([[[4]], [[4]]], [[[4]], [[4]]])},\n",
    "    subspaces=[[[1]], [[0]]],\n",
    ")\n",
    "print(torch.equal(pv_out3.last_hidden_state, pv_out4.last_hidden_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5b7a3e",
   "metadata": {},
   "source": [
    "### Add New Model Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acce6e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "\n",
    "# get a flan-t5 from HuggingFace\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "config = T5Config.from_pretrained(\"google/flan-t5-small\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(\n",
    "    \"google/flan-t5-small\", config=config\n",
    ")\n",
    "\n",
    "# config the intervention mapping with pv global vars\n",
    "\"\"\"Only define for the block output here for simplicity\"\"\"\n",
    "pv.type_to_module_mapping[type(t5)] = {\n",
    "    \"mlp_output\": (\"encoder.block[%s].layer[1]\", \n",
    "                   pv.models.constants.CONST_OUTPUT_HOOK),\n",
    "    \"attention_input\": (\"encoder.block[%s].layer[0]\", \n",
    "                        pv.models.constants.CONST_OUTPUT_HOOK),\n",
    "}\n",
    "pv.type_to_dimension_mapping[type(t5)] = {\n",
    "    \"mlp_output\": (\"d_model\",),\n",
    "    \"attention_input\": (\"d_model\",),\n",
    "    \"block_output\": (\"d_model\",),\n",
    "    \"head_attention_value_output\": (\"d_model/num_heads\",),\n",
    "}\n",
    "\n",
    "# wrap as gpt2\n",
    "pv_t5 = pv.IntervenableModel({\n",
    "    \"layer\": 0,\n",
    "    \"component\": \"mlp_output\",\n",
    "    \"source_representation\": torch.zeros(\n",
    "        t5.config.d_model)\n",
    "}, model=t5)\n",
    "\n",
    "# then intervene!\n",
    "base = tokenizer(\"The capital of Spain is\", \n",
    "                 return_tensors=\"pt\")\n",
    "decoder_input_ids = tokenizer(\n",
    "    \"\", return_tensors=\"pt\").input_ids\n",
    "base[\"decoder_input_ids\"] = decoder_input_ids\n",
    "intervened_outputs = pv_t5(\n",
    "    base, \n",
    "    unit_locations={\"base\": 3}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba158a92",
   "metadata": {},
   "source": [
    "### Composing Complex Intervention Schema: Path Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51cadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "\n",
    "def path_patching_config(\n",
    "    layer, last_layer, \n",
    "    component=\"head_attention_value_output\", unit=\"h.pos\"\n",
    "):\n",
    "    intervening_component = [\n",
    "        {\"layer\": layer, \"component\": component, \"unit\": unit, \"group_key\": 0}]\n",
    "    restoring_components = []\n",
    "    if not component.startswith(\"mlp_\"):\n",
    "        restoring_components += [\n",
    "            {\"layer\": layer, \"component\": \"mlp_output\", \"group_key\": 1}]\n",
    "    for i in range(layer+1, last_layer):\n",
    "        restoring_components += [\n",
    "            {\"layer\": i, \"component\": \"attention_output\", \"group_key\": 1},\n",
    "            {\"layer\": i, \"component\": \"mlp_output\", \"group_key\": 1}\n",
    "        ]\n",
    "    intervenable_config = pv.IntervenableConfig(\n",
    "        intervening_component + restoring_components)\n",
    "    return intervenable_config\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    path_patching_config(4, gpt2.config.n_layer), \n",
    "    model=gpt2\n",
    ")\n",
    "\n",
    "pv_gpt2.save(\n",
    "    save_directory=\"./tmp/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "pv_gpt2 = pv.IntervenableModel.load(\n",
    "    \"./tmp/\",\n",
    "    model=gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546e858",
   "metadata": {},
   "source": [
    "### Composing Complex Intervention Schema: Causal Tracing in 15 lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvene as pv\n",
    "\n",
    "def causal_tracing_config(\n",
    "  l, c=\"mlp_activation\", w=10, tl=48):\n",
    "  s = max(0, l - w // 2)\n",
    "  e = min(tl, l - (-w // 2))\n",
    "  config = pv.IntervenableConfig(\n",
    "    [{\"component\": \"block_input\"}] + \n",
    "    [{\"layer\": l, \"component\": c} \n",
    "      for l in range(s, e)],\n",
    "    [pv.NoiseIntervention] +\n",
    "    [pv.VanillaIntervention]*(e-s))\n",
    "  return config\n",
    "\n",
    "_, tokenizer, gpt2 = pv.create_gpt2()\n",
    "\n",
    "pv_gpt2 = pv.IntervenableModel(\n",
    "    causal_tracing_config(4), \n",
    "    model=gpt2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6eb49d",
   "metadata": {},
   "source": [
    "### The End\n",
    "Now you are graduating from pyvene 101! Feel free to take a look at our tutorials for more challenging interventions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
