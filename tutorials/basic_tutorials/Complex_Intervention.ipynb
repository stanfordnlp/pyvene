{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f999c6ab",
   "metadata": {},
   "source": [
    "## Tutorial of More Complex Interventions Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbd34970",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zhengxuan Wu\"\n",
    "__version__ = \"12/19/2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a0f275",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The basic tutorials cover simple usages of interventions. Here, we showcase some more advance usages of this library, which can support flexible interventions by grouping interventions together, skipping interventions when needed, etc... This is a live tutorial which encapsulates a set of advanced usages together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46365d6f",
   "metadata": {},
   "source": [
    "### Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b59e7680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-11 01:22:07,607] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyvene\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install git+https://github.com/frankaging/pyvene.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06712c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pyvene import embed_to_distrib, top_vals, format_token\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    VanillaIntervention, LowRankRotatedSpaceIntervention,\n",
    "    IntervenableRepresentationConfig,\n",
    "    IntervenableConfig,\n",
    ")\n",
    "from pyvene import create_gpt2\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "from plotnine import (\n",
    "    ggplot,\n",
    "    geom_tile,\n",
    "    aes,\n",
    "    facet_wrap,\n",
    "    theme,\n",
    "    element_text,\n",
    "    geom_bar,\n",
    "    geom_hline,\n",
    "    scale_y_log10,\n",
    ")\n",
    "\n",
    "config, tokenizer, gpt = create_gpt2(cache_dir=\"../../../.huggingface_cache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a2e9ee",
   "metadata": {},
   "source": [
    "### Non-group-based Interventions v.s. Group-based Interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e3b92",
   "metadata": {},
   "source": [
    "Two same sources are used to intervene at two locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024687a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervenable_config = IntervenableConfig(\n",
    "    intervenable_model_type=type(gpt),\n",
    "    intervenable_representations=[\n",
    "        IntervenableRepresentationConfig(\n",
    "            0,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "        ),\n",
    "        IntervenableRepresentationConfig(\n",
    "            2,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "        ),\n",
    "    ],\n",
    "    intervenable_interventions_type=VanillaIntervention,\n",
    ")\n",
    "intervenable = IntervenableModel(intervenable_config, gpt)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [\n",
    "    tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"),\n",
    "    tokenizer(\"The capital of Italy is\", return_tensors=\"pt\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafd77cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counterfactual_outputs_no_group = intervenable(\n",
    "    base, sources, {\"sources->base\": ([[[3]], [[4]]], [[[3]], [[4]]])}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17dafda",
   "metadata": {},
   "source": [
    "One single source is used for all interventions in the group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bab4e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervenable_config = IntervenableConfig(\n",
    "    intervenable_model_type=type(gpt),\n",
    "    intervenable_representations=[\n",
    "        IntervenableRepresentationConfig(0, \"block_output\", \"pos\", 1, group_key=0),\n",
    "        IntervenableRepresentationConfig(2, \"block_output\", \"pos\", 1, group_key=0),\n",
    "    ],\n",
    "    intervenable_interventions_type=VanillaIntervention,\n",
    ")\n",
    "intervenable = IntervenableModel(intervenable_config, gpt)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "sources = [tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e12c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counterfactual_outputs_group = intervenable(\n",
    "    base, sources, {\"sources->base\": ([[[3]], [[4]]], [[[3]], [[4]]])}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfaab70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.equal(\n",
    "    counterfactual_outputs_no_group.last_hidden_state,\n",
    "    counterfactual_outputs_group.last_hidden_state,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746cfaaa",
   "metadata": {},
   "source": [
    "### Smart skipping interventions by passing in None\n",
    "\n",
    "This library respects the intervention list as the source of the truth when accepting different inputs. However, sometimes, we may only need to intervene on a partial list of all listed interventions. We can do that by passing in None in the source input list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26df3873",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervenable_config = IntervenableConfig(\n",
    "    intervenable_model_type=type(gpt),\n",
    "    intervenable_representations=[\n",
    "        IntervenableRepresentationConfig(\n",
    "            0,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "        ),\n",
    "        IntervenableRepresentationConfig(\n",
    "            0,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "        ),\n",
    "        IntervenableRepresentationConfig(\n",
    "            0,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "        ),\n",
    "    ],\n",
    "    intervenable_interventions_type=VanillaIntervention,\n",
    ")\n",
    "intervenable = IntervenableModel(intervenable_config, gpt)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2afdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counterfactual_outputs_1 = intervenable(\n",
    "    base,\n",
    "    [None, None, source],\n",
    "    {\"sources->base\": ([None, None, [[4]]], [None, None, [[4]]])},\n",
    ")\n",
    "_, counterfactual_outputs_2 = intervenable(\n",
    "    base,\n",
    "    [None, source, None],\n",
    "    {\"sources->base\": ([None, [[4]], None], [None, [[4]], None])},\n",
    ")\n",
    "_, counterfactual_outputs_3 = intervenable(\n",
    "    base,\n",
    "    [source, None, None],\n",
    "    {\"sources->base\": ([[[4]], None, None], [[[4]], None, None])},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7b714b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.equal(\n",
    "        counterfactual_outputs_1.last_hidden_state,\n",
    "        counterfactual_outputs_2.last_hidden_state,\n",
    "    ),\n",
    "    torch.equal(\n",
    "        counterfactual_outputs_2.last_hidden_state,\n",
    "        counterfactual_outputs_3.last_hidden_state,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90760a9f",
   "metadata": {},
   "source": [
    "### Weight-sharing interventions targetting different subspaces\n",
    "\n",
    "Trainable interventions also support weight sharing. This is useful if two interventions are targetting different subspaces of a new basis. This is different from one intervention with paritioned subspaces. The latter case only allow intervening at one subspace at a time, which could be useful as well. However, weight-sharing with smart skipping may be suffice for all the use-cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b635e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "intervenable_config = IntervenableConfig(\n",
    "    intervenable_model_type=type(gpt),\n",
    "    intervenable_representations=[\n",
    "        IntervenableRepresentationConfig(\n",
    "            0,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "            intervenable_low_rank_dimension=2,\n",
    "            subspace_partition=[[0, 1], [1, 2]],\n",
    "            intervention_link_key=0,  # create sym link across interventions\n",
    "        ),\n",
    "        IntervenableRepresentationConfig(\n",
    "            0,\n",
    "            \"block_output\",\n",
    "            \"pos\",\n",
    "            1,\n",
    "            intervenable_low_rank_dimension=2,\n",
    "            subspace_partition=[[0, 1], [1, 2]],\n",
    "            intervention_link_key=0,  # create sym link across interventions\n",
    "        ),\n",
    "    ],\n",
    "    intervenable_interventions_type=LowRankRotatedSpaceIntervention,\n",
    ")\n",
    "intervenable = IntervenableModel(intervenable_config, gpt)\n",
    "\n",
    "base = tokenizer(\"The capital of Spain is\", return_tensors=\"pt\")\n",
    "source = tokenizer(\"The capital of Italy is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8bcecb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, counterfactual_outputs_1 = intervenable(\n",
    "    base,\n",
    "    [None, source],\n",
    "    {\"sources->base\": ([None, [[4]]], [None, [[4]]])},\n",
    "    subspaces=[None, [[1]]],\n",
    ")\n",
    "_, counterfactual_outputs_2 = intervenable(\n",
    "    base,\n",
    "    [source, None],\n",
    "    {\"sources->base\": ([[[4]], None], [[[4]], None])},\n",
    "    subspaces=[[[1]], None],\n",
    ")\n",
    "_, counterfactual_outputs_3 = intervenable(\n",
    "    base,\n",
    "    [source, source],\n",
    "    {\"sources->base\": ([[[4]], [[4]]], [[[4]], [[4]]])},\n",
    "    subspaces=[[[0]], [[1]]],\n",
    ")\n",
    "_, counterfactual_outputs_4 = intervenable(\n",
    "    base,\n",
    "    [source, source],\n",
    "    {\"sources->base\": ([[[4]], [[4]]], [[[4]], [[4]]])},\n",
    "    subspaces=[[[1]], [[0]]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2c282a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False False True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    torch.equal(\n",
    "        counterfactual_outputs_1.last_hidden_state,\n",
    "        counterfactual_outputs_2.last_hidden_state,\n",
    "    ),\n",
    "    torch.equal(\n",
    "        counterfactual_outputs_2.last_hidden_state,\n",
    "        counterfactual_outputs_3.last_hidden_state,\n",
    "    ),\n",
    "    torch.allclose(\n",
    "        counterfactual_outputs_1.last_hidden_state,\n",
    "        counterfactual_outputs_3.last_hidden_state,\n",
    "        atol=1e-5,  # bmm in different order will result in slightly different results\n",
    "    ),\n",
    "    torch.allclose(\n",
    "        counterfactual_outputs_3.last_hidden_state,\n",
    "        counterfactual_outputs_4.last_hidden_state,\n",
    "        atol=1e-5,  # bmm in different order will result in slightly different results\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f9daf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counterfactual_outputs_4[0].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a061b54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.8147e-06)\n"
     ]
    }
   ],
   "source": [
    "# this is an example about order matters for percision\n",
    "x = torch.randn(10, 10, 10)\n",
    "s1 = x.sum()\n",
    "s2 = x.sum(0).sum(0).sum(0)\n",
    "print((s1 - s2).abs().max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
