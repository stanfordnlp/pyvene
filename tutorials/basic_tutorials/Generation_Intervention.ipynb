{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded GPTNeo model roneneldan/TinyStories-33M\n",
      "GPTNeoForCausalLM(\n",
      "  (transformer): GPTNeoModel(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(2048, 768)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-3): 4 x GPTNeoBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPTNeoAttention(\n",
      "          (attention): GPTNeoSelfAttention(\n",
      "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPTNeoMLP(\n",
      "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pyvene as pv\n",
    "import pprint\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "config, tokenizer, model = pv.create_gpt_neo()\n",
    "model.to(DEVICE)\n",
    "pprint.pprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2, 384])\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "collect_model = pv.IntervenableModel(\n",
    "    [\n",
    "        {\n",
    "            \"layer\": l,\n",
    "            \"component\": \"block_output\",\n",
    "            \"intervention_type\": pv.CollectIntervention,\n",
    "        }\n",
    "        for l in range(1, config.num_layers)\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "p_plus = \" love\"\n",
    "p_minus = \" hate\"\n",
    "\n",
    "res = collect_model(\n",
    "    base=tokenizer([p_plus, p_minus], return_tensors=\"pt\").to(DEVICE),\n",
    "    unit_locations={\"base\": 0},\n",
    "    return_dict=True,\n",
    ")[\"collected_activations\"]\n",
    "\n",
    "print(res['layer.1.comp.block_output.unit.pos.nunit.1#0'].shape)\n",
    "print(config.num_heads)\n",
    "\n",
    "diff: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "for k, v in res.items():\n",
    "    diff[k] = torch.reshape(res[k][0] - res[k][1], (-1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unintervened generation:'\n",
      "['I hate you because I don\\'t want you to be my friend.\"\\n'\n",
      " '\\n'\n",
      " 'The little girl was sad and went home. She told her mom what happened and']\n",
      "Intervened generation:\n",
      "['I hate you because batch of cookies are not good for you. You are a bad '\n",
      " \"sister. I don't want to play with you anymore. I want to\"]\n"
     ]
    }
   ],
   "source": [
    "intv_model = pv.IntervenableModel(\n",
    "    [\n",
    "        {\n",
    "            \"layer\": l,\n",
    "            \"component\": \"block_output\",\n",
    "            \"intervention\": lambda b, s: b + 10 * s,\n",
    "        }\n",
    "        for l in range(1, config.num_layers)\n",
    "    ],\n",
    "    model=model,\n",
    ")\n",
    "\n",
    "# ActAdd on prompt (original setting)\n",
    "orig, intervened = intv_model.generate(\n",
    "    base=tokenizer(\"I hate you because\", return_tensors=\"pt\").to(DEVICE),\n",
    "    source_representations=diff,\n",
    "    unit_locations={\"sources->base\": (0, 3)},\n",
    "    output_original_output=True,\n",
    "    max_length=32,\n",
    ")\n",
    "\n",
    "pprint.pprint('Unintervened generation:')\n",
    "pprint.pprint(tokenizer.batch_decode(orig))\n",
    "\n",
    "print('Intervened generation:')\n",
    "pprint.pprint(tokenizer.batch_decode(intervened))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Unintervened generation:'\n",
      "['I hate you because I don\\'t want you to be my friend.\"\\n'\n",
      " '\\n'\n",
      " 'The little girl was sad and went home. She told her mom what happened and '\n",
      " 'her mom said, \"Don\\'t worry, we can make a new friend tomorrow.\" The']\n",
      "Intervened generation:\n",
      "['I hate you because I don\\'t want you to be my friend.\" batch of cookies and '\n",
      " 'they both laughed. The end batch of cookies were so delicious that they made '\n",
      " 'the batch of cookies and they both ate them together. batch of cookies were '\n",
      " 'so tasty']\n"
     ]
    }
   ],
   "source": [
    "# ActAdd on decoded region\n",
    "orig, intervened = intv_model.generate(\n",
    "    base=tokenizer(\"I hate you because\", return_tensors=\"pt\").to(DEVICE),\n",
    "    source_representations=diff,\n",
    "    unit_locations={\"sources->base\": (0, 3)},\n",
    "    intervene_on_prompt=False,\n",
    "    timestep_selector=[lambda idx, o: idx % 10 == 0 for i in range(3)],\n",
    "    output_original_output=True,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "pprint.pprint('Unintervened generation:')\n",
    "pprint.pprint(tokenizer.batch_decode(orig))\n",
    "\n",
    "print('Intervened generation:')\n",
    "pprint.pprint(tokenizer.batch_decode(intervened))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
