{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14c34758",
   "metadata": {},
   "source": [
    "## Tutorial of changing how TinyStories end with simple interventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88fd28a",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/frankaging/align-transformers/blob/main/tutorials/Change%20how%20TinyStories%20end%20with%20interventions.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aa67223",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Zhengxuan Wu\"\n",
    "__version__ = \"10/08/2023\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496303f3",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Most of the tutorials focus on a single token generation task (e.g., capital change, price tagging task). Most of the real-world tasks are multi-token generations (e.g., ChatGPT). In this tutorial, we show how to intervene on a generic language generation task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b918c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import transformers\n",
    "    import sys\n",
    "    sys.path.append(\"align-transformers/\")\n",
    "except ModuleNotFoundError:\n",
    "    !git clone https://github.com/frankaging/align-transformers.git\n",
    "    !pip install -r align-transformers/requirements.txt\n",
    "    import sys\n",
    "    sys.path.append(\"align-transformers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa6a75e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-14 17:52:15,227] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from models.utils import print_forward_hooks, remove_forward_hooks\n",
    "from models.configuration_alignable_model import AlignableRepresentationConfig, AlignableConfig\n",
    "from models.alignable_base import AlignableModel\n",
    "from models.interventions import AdditionIntervention, SubstractionIntervention\n",
    "from models.gpt_neo.modelings_alignable_gpt_neo import create_gpt_neo\n",
    "\n",
    "%config InlineBackend.figure_formats = ['svg']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bd9c6",
   "metadata": {},
   "source": [
    "### Original generation with TinyStories-33M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354ac7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "config, tokenizer, tinystory = create_gpt_neo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e2e363d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw a big, red balloon. She was so excited and ran over to it.\n",
      "\n",
      "\"Can I have it?\" she asked.\n",
      "\n",
      "\"No,\" said her mom. \"It's too big for you. You can't have it.\"\n",
      "\n",
      "Lucy was sad, but then she saw a small, red balloon. She smiled and said, \"I want that one!\"\n",
      "\n",
      "Her mom smiled and said, \"Okay, let's go get it.\"\n",
      "\n",
      "So they went to the balloon and Lucy was so happy. She held the balloon tight and ran around the park with it. She laughed and smiled and had so much fun.\n",
      "\n",
      "When it was time to go home, Lucy hugged the balloon and said, \"I love you, balloon!\"\n",
      "\n",
      "Her mom smiled and said, \"I love you too, Lucy.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Once upon a time there was\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "output = tinystory.generate(input_ids, max_length = 512, num_beams=1)\n",
    "output_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e25cf",
   "metadata": {},
   "source": [
    "### Set-up interventions on language generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22544382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "def activation_addition_position_config(\n",
    "    model_type, intervention_type, \n",
    "    start_layer_idx, end_layer_idx\n",
    "):\n",
    "    alignable_config = AlignableConfig(\n",
    "        alignable_model_type=model_type,\n",
    "        alignable_representations=[\n",
    "            AlignableRepresentationConfig(\n",
    "                i,                 # layer\n",
    "                intervention_type, # intervention type\n",
    "                \"pos\",             # intervention unit\n",
    "                1                  # max number of unit\n",
    "            ) for i in range(start_layer_idx, end_layer_idx)\n",
    "        ],\n",
    "        alignable_interventions_type=AdditionIntervention,\n",
    "    )\n",
    "    return alignable_config\n",
    "\n",
    "config, tokenizer, tinystory = create_gpt_neo()\n",
    "alignable_config = activation_addition_position_config(\n",
    "    type(tinystory), \"mlp_output\", 0, 4\n",
    ")\n",
    "alignable = AlignableModel(alignable_config, tinystory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b835b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer.0.repr.mlp_output.unit.pos.nunit.1#0',\n",
       " 'layer.1.repr.mlp_output.unit.pos.nunit.1#0',\n",
       " 'layer.2.repr.mlp_output.unit.pos.nunit.1#0',\n",
       " 'layer.3.repr.mlp_output.unit.pos.nunit.1#0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sad_token_id = tokenizer(\" Sad\")[\"input_ids\"][0]\n",
    "happy_token_id = tokenizer(\" Happy\")[\"input_ids\"][0]\n",
    "\n",
    "beta = 0.3 # very hacky way to control the effect\n",
    "sad_embedding = tinystory.transformer.wte(\n",
    "    torch.tensor(sad_token_id)).clone().unsqueeze(0).unsqueeze(0) # make it a fake batch\n",
    "happy_embedding = tinystory.transformer.wte(\n",
    "    torch.tensor(happy_token_id)).clone().unsqueeze(0).unsqueeze(0) # make it a fake batch\n",
    "sad_embedding *= beta\n",
    "happy_embedding *= beta\n",
    "\n",
    "activations_sad_sources = dict(\n",
    "    zip(alignable.sorted_alignable_keys, \n",
    "        [sad_embedding]*len(alignable.sorted_alignable_keys))\n",
    ")\n",
    "activations_happy_sources = dict(\n",
    "    zip(alignable.sorted_alignable_keys, \n",
    "        [happy_embedding]*len(alignable.sorted_alignable_keys))\n",
    ")\n",
    "# we intervene on all of the mlp output\n",
    "alignable.sorted_alignable_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eaf4ac",
   "metadata": {},
   "source": [
    "### Adding a little bit of \"sadness\" into the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25d5f3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw a big, red balloon. She was so excited and ran over to it. She reached out to grab it, but it was too high.\n",
      "\n",
      "Suddenly, a kind old man appeared. He said, \"I can help you get the balloon, but first you must do something for me.\" Lucy was confused, but she agreed. The old man said, \"I need you to help me pull the balloon down from the tree.\"\n",
      "\n",
      "Lucy was scared, but she was brave and she did it. The old man grabbed the balloon and handed it to Lucy. She was so happy and thanked the old man.\n",
      "\n",
      "The old man said, \"You are very brave and strong. I'm glad I could help you.\" Lucy smiled and said, \"Thank you!\" She hugged the balloon tightly and ran off to show her mom her new prize.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = \"Once upon a time there was\"\n",
    "\n",
    "inputs = tokenizer(base, return_tensors=\"pt\")\n",
    "base_outputs, counterfactual_outputs = alignable.generate(\n",
    "    inputs, \n",
    "    unit_locations={\n",
    "        \"sources->base\": (\n",
    "            [[[0]]]*tinystory.config.num_layers, # a single token embeddings\n",
    "            [[[0]]]*tinystory.config.num_layers  # the last token of the prompt\n",
    "        )\n",
    "    },\n",
    "    activations_sources=activations_sad_sources,\n",
    "    intervene_on_prompt=False,\n",
    "    max_length=256, num_beams=1,\n",
    ")\n",
    "counterfactual_text = tokenizer.decode(\n",
    "    counterfactual_outputs[0], skip_special_tokens=True)\n",
    "print(counterfactual_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50357fc",
   "metadata": {},
   "source": [
    "### Adding a little bit of \"happiness\" into the story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f95a802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time there was a little girl named Lucy. She was three years old and loved to explore. One day, Lucy was walking in the park when she saw a big, red balloon. She was so excited and wanted to play with it.\n",
      "\n",
      "But then, a big, mean man came and said, \"That balloon is mine! You can't have it!\" Lucy was very sad and started to cry.\n",
      "\n",
      "The man said, \"I'm sorry, but I need the balloon for my work. You can have it if you want.\"\n",
      "\n",
      "Lucy was so happy and said, \"Yes please!\" She took the balloon and ran away.\n",
      "\n",
      "But then, the man said, \"Wait! I have an idea. Let's make a deal. If you can guess what I'm going to give you, then you can have the balloon.\"\n",
      "\n",
      "Lucy thought for a moment and then said, \"I guess I'll have to get the balloon.\"\n",
      "\n",
      "The man smiled and said, \"That's a good guess! Here you go.\"\n",
      "\n",
      "Lucy was so happy and thanked the man. She hugged the balloon and ran off to show her mom.\n",
      "\n",
      "The end.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base = \"Once upon a time there was\"\n",
    "\n",
    "inputs = tokenizer(base, return_tensors=\"pt\")\n",
    "base_outputs, counterfactual_outputs = alignable.generate(\n",
    "    inputs, \n",
    "    unit_locations={\n",
    "        \"sources->base\": (\n",
    "            [[[0]]]*tinystory.config.num_layers, # a single token embeddings\n",
    "            [[[0]]]*tinystory.config.num_layers  # the last token of the prompt\n",
    "        )\n",
    "    },\n",
    "    activations_sources=activations_happy_sources,\n",
    "    intervene_on_prompt=False,\n",
    "    max_length=256, num_beams=1,\n",
    ")\n",
    "counterfactual_text = tokenizer.decode(\n",
    "    counterfactual_outputs[0], skip_special_tokens=True)\n",
    "print(counterfactual_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
